

# Comparative Analysis of Synchronization Solutions for LeetCode 1114: Print in Order

1. [Intoduction](#id1)
2. [Problem Overview](#id2)
3. [Code Skeleton](#id3)
4. [Synchronization Solutions](#id4)
    1. [Barrier Solution](#id41)
    2. [Lock Solution](#id42)
    3. [Event Solution](#id43)
    4. [Semaphore Solution](#id44)
    5. [Condition Solution](#id45)
5. [Performance Comparison](#id5)
    1. [Time Complexity](#id51)
    2. [Space Complexity](#id52)
    3. [Synchronization Overhead](#id53)
    4. [Empirical Performance Considerations](#id54)
    5. [Performance Comparison Table](#id55)
6. [Use Case Suitability](#id6)
7. [Conclusion](#id7)

## Introduction <a id="id1"></a>
LeetCode problem 1114, "Print in Order," is a concurrency challenge requiring three threads to execute methods `first()`, `second()`, and `third()` in a specific sequence, producing the output "firstsecondthird" regardless of thread scheduling. This article analyzes five Python threading solutions—Barrier, Lock, Event, Semaphore, and Condition—provided by Alex M. We provide detailed explanations of each method’s mechanism, implementation, advantages, disadvantages, and extended insights into their behavior, followed by a comprehensive performance comparison with a tabular summary.

## Problem Overview <a id="id2"></a>
The problem involves a `Foo` class with three methods: `first()`, `second()`, and `third()`, each called by a separate thread (A, B, and C). The methods print "first," "second," and "third," respectively. The challenge is to ensure `second()` executes after `first()`, and `third()` after `second()`, despite unpredictable thread scheduling. The input, a permutation of [1, 2, 3], indicates the call order, but the output must always be "firstsecondthird."

---

Suppose we have a class:

```
public class Foo {
  public void first() { print("first"); }
  public void second() { print("second"); }
  public void third() { print("third"); }
}
```

The same instance of Foo will be passed to three different threads. Thread A will call first(), thread B will call second(), and thread C will call third(). Design a mechanism and modify the program to ensure that second() is executed after first(), and third() is executed after second().

**Note**:
We do not know how the threads will be scheduled in the operating system, even though the numbers in the input seem to imply the ordering. The input format you see is mainly to ensure our tests' comprehensiveness.

 

>Example 1: <br>
>
>Input: nums = [1,2,3] <br>
Output: "firstsecondthird" <br>
Explanation: There are three threads being fired asynchronously. The input [1,2,3] means thread A calls first(), thread B calls second(), and thread C calls third(). "firstsecondthird" is the correct output.


>Example 2: <br>
>
>Input: nums = [1,3,2] <br>
Output: "firstsecondthird" <br>
Explanation: The input [1,3,2] means thread A calls first(), thread B calls third(), and thread C calls second(). "firstsecondthird" is the correct output.

---

## Code Skeleton <a id="id3"></a>
```python
class Foo:
    def __init__(self):
        pass


    def first(self, printFirst: 'Callable[[], None]') -> None:
        
        # printFirst() outputs "first". Do not change or remove this line.
        printFirst()


    def second(self, printSecond: 'Callable[[], None]') -> None:
        
        # printSecond() outputs "second". Do not change or remove this line.
        printSecond()


    def third(self, printThird: 'Callable[[], None]') -> None:
        
        # printThird() outputs "third". Do not change or remove this line.
        printThird()
```


## Synchronization Solutions <a id="id4"></a>
Below, we describe each solution, its mechanism, implementation, and extended details, including thread interaction, edge cases, and potential pitfalls.

### 1. Barrier Solution <a id="id41"></a>
**Mechanism**: Utilizes two `Barrier` objects, each requiring two threads to reach a synchronization point before proceeding. The first thread (`first()`) prints and waits at the first barrier, synchronizing with the second thread (`second()`), which waits, prints, and then waits at the second barrier. The third thread (`third()`) waits at the second barrier and prints after synchronization. Barriers ensure that threads rendezvous at specific points, enforcing the sequence.

**Implementation**:
```python
from threading import Barrier

class Foo:
    def __init__(self):
        self.first_barrier = Barrier(2)
        self.second_barrier = Barrier(2)
            
    def first(self, printFirst):
        printFirst()
        self.first_barrier.wait()
        
    def second(self, printSecond):
        self.first_barrier.wait()
        printSecond()
        self.second_barrier.wait()
            
    def third(self, printThird):
        self.second_barrier.wait()
        printThird()
```

**Extended Details**:
- **Thread Interaction**: The first barrier synchronizes threads A and B, ensuring `second()` waits for `first()` to complete. The second barrier synchronizes threads B and C, ensuring `third()` waits for `second()`. Each barrier acts as a gate, opening only when both threads arrive.
- **Edge Cases**: If a thread fails or is delayed significantly, the barrier may cause a deadlock, as the other thread waits indefinitely. Barriers are sensitive to the exact number of threads (two in this case), making them inflexible for dynamic thread counts.
- **Pitfalls**: Overuse of barriers can lead to unnecessary synchronization points, increasing latency. Developers must ensure the correct number of threads reach each barrier to avoid hangs.
- **Real-World Use**: Barriers are ideal for scenarios like parallel computations where threads must align at intermediate stages (e.g., matrix operations in scientific computing).

**Advantages**:
- **Simplicity**: Intuitive for pairwise synchronization, resembling a meeting point for threads.
- **Clear Control Flow**: Barriers explicitly define synchronization points, making the sequence easy to follow.
- **No State Management**: Unlike other methods, barriers don’t require explicit state variables.

**Disadvantages**:
- **Fixed Thread Count**: Requires exactly two threads per barrier, limiting adaptability.
- **Overhead**: Managing two barriers introduces complexity for a simple linear sequence.
- **Deadlock Risk**: Failure of one thread to reach the barrier can stall others.

### 2. Lock Solution <a id="id42"></a>
**Mechanism**: Employs two `Lock` objects, initially acquired (locked) in the constructor. The first thread releases the first lock after printing, allowing the second thread to acquire it and print. The second thread releases the second lock, enabling the third thread. Locks act as gates, opened sequentially by each thread.

**Implementation**:
```python
from threading import Lock

class Foo:
    def __init__(self):
        self.locks = (Lock(), Lock())
        self.locks[0].acquire()
        self.locks[1].acquire()
        
    def first(self, printFirst):
        printFirst()
        self.locks[0].release()
        
    def second(self, printSecond):
        with self.locks[0]:
            printSecond()
            self.locks[1].release()
            
    def third(self, printThird):
        with self.locks[1]:
            printThird()
```

**Extended Details**:
- **Thread Interaction**: The first thread (A) releases the first lock, unblocking thread B, which waits on it. Thread B then releases the second lock, unblocking thread C. The use of `with` statements ensures automatic lock release, reducing errors.
- **Edge Cases**: If the first thread fails to release the lock, threads B and C will block indefinitely. The pre-acquired lock approach ensures immediate blocking for threads B and C, avoiding race conditions.
- **Pitfalls**: Pre-acquiring locks is unconventional and may confuse developers expecting locks to start unlocked. Incorrect lock ordering in more complex systems could lead to deadlocks.
- **Real-World Use**: Locks are common in resource protection (e.g., database access) and simple sequential tasks, such as coordinating pipeline stages in a processing system.

**Advantages**:
- **Lightweight**: Locks have minimal overhead, making them efficient.
- **Explicit Control**: The acquire/release pattern clearly enforces the order.
- **Robustness**: Simple mechanism reduces the likelihood of synchronization errors.

**Disadvantages**:
- **Initial Lock State**: Pre-acquiring locks is non-standard and may reduce code readability.
- **Deadlock Risk**: Improper lock management in complex scenarios could cause deadlocks.
- **Limited Flexibility**: Locks are less suited for complex synchronization patterns.

### 3. Event Solution <a id="id43"></a>
**Mechanism**: Uses two `Event` objects to signal task completion. The first thread sets the first event after printing, allowing the second thread to proceed after waiting. The second thread sets the second event, enabling the third thread. Events act as flags, signaling when a task is done.

**Implementation**:
```python
from threading import Event

class Foo:
    def __init__(self):
        self.done = (Event(), Event())
        
    def first(self, printFirst):
        printFirst()
        self.done[0].set()
        
    def second(self, printSecond):
        self.done[0].wait()
        printSecond()
        self.done[1].set()
            
    def third(self, printThird):
        self.done[1].wait()
        printThird()
```

**Extended Details**:
- **Thread Interaction**: Thread A sets the first event, unblocking thread B, which waits on it. Thread B sets the second event, unblocking thread C. The `wait()` method suspends threads until the event is set, minimizing CPU usage.
- **Edge Cases**: If the first thread fails to set the event, subsequent threads block indefinitely. Events are one-time signals in this context, eliminating the need for reset logic.
- **Pitfalls**: Polling in Python’s `wait()` implementation may introduce latency. Developers must ensure events are set in the correct order to avoid hangs.
- **Real-World Use**: Events are used in producer-consumer patterns (e.g., task queues) and event-driven systems, such as GUI frameworks or network servers.

**Advantages**:
- **Intuitive Signaling**: Events provide a clear way to signal completion.
- **Flexibility**: Not tied to a fixed number of threads, unlike barriers.
- **Ease of Use**: Simple set/wait API reduces complexity.

**Disadvantages**:
- **Polling Overhead**: Waiting may involve polling, increasing latency.
- **No Reusability**: Events are one-time signals unless manually reset.
- **Potential Latency**: Polling can delay response in high-contention scenarios.

### 4. Semaphore Solution <a id="id44"></a>
**Mechanism**: Uses two `Semaphore` objects initialized to 0 (closed gates). The first thread releases the first semaphore after printing, allowing the second thread to acquire it and print. The second thread releases the second semaphore, enabling the third thread. Semaphores act as counters, controlling access to the next stage.

**Implementation**:
```python
from threading import Semaphore

class Foo:
    def __init__(self):
        self.gates = (Semaphore(0), Semaphore(0))
        
    def first(self, printFirst):
        printFirst()
        self.gates[0].release()
        
    def second(self, printSecond):
        with self.gates[0]:
            printSecond()
            self.gates[1].release()
            
    def third(self, printThird):
        with self.gates[1]:
            printThird()
```

**Extended Details**:
- **Thread Interaction**: Thread A releases the first semaphore, incrementing its counter and unblocking thread B. Thread B releases the second semaphore, unblocking thread C. The `with` statement ensures proper acquisition and release.
- **Edge Cases**: If the first thread fails to release the semaphore, subsequent threads block. The zero-initialized semaphores ensure threads wait until explicitly released.
- **Pitfalls**: Semaphores are more complex than locks due to their counting nature, which is unnecessary here. Incorrect release counts in other contexts could lead to bugs.
- **Real-World Use**: Semaphores are used for resource pooling (e.g., limiting database connections) or coordinating multiple threads in parallel tasks.

**Advantages**:
- **Flexible Counting**: Supports counting, useful for resource management.
- **Context Manager Support**: Simplifies acquisition/release with `with` statements.
- **Robustness**: Clear release/acquire pattern ensures order.

**Disadvantages**:
- **Complexity**: Semaphores are more complex than locks for this use case.
- **Initial State**: Zero-value semaphores are less intuitive.
- **Overkill**: Counting capability is unused in this single-release scenario.

### 5. Condition Solution <a id="id45"></a>
**Mechanism**: Uses a `Condition` object with an RLock and a shared `order` variable. The first thread prints, sets `order` to 1, and notifies others. The second thread waits until `order` is 1, prints, and sets `order` to 2. The third thread waits until `order` is 2 and prints. Conditions allow threads to wait on specific predicates.

**Implementation**:
```python
from threading import Condition

class Foo:
    def __init__(self):
        self.exec_condition = Condition()
        self.order = 0
        self.first_finish = lambda: self.order == 1
        self.second_finish = lambda: self.order == 2

    def first(self, printFirst):
        with self.exec_condition:
            printFirst()
            self.order = 1
            self.exec_condition.notify(2)

    def second(self, printSecond):
        with self.exec_condition:
            self.exec_condition.wait_for(self.first_finish)
            printSecond()
            self.order = 2
            self.exec_condition.notify()

    def third(self, printThird):
        with self.exec_condition:
            self.exec_condition.wait_for(self.second_finish)
            printThird()
```

**Extended Details**:
- **Thread Interaction**: Thread A acquires the condition’s lock, prints, updates `order`, and notifies up to two waiting threads. Thread B waits until `order` is 1, then proceeds. Thread C waits until `order` is 2. The `wait_for` method checks predicates, releasing the lock while waiting.
- **Edge Cases**: If notifications are missed (e.g., due to early calls), threads may block indefinitely. The `order` variable ensures correct sequencing, but incorrect state updates could cause issues.
- **Pitfalls**: Conditions require careful state management and notification counts. Over-notification is harmless, but under-notification can lead to deadlocks.
- **Real-World Use**: Conditions are used in complex workflows, such as task scheduling systems or state machines, where threads wait on specific conditions.

**Advantages**:
- **Fine-Grained Control**: Supports complex predicates for synchronization.
- **Reusability**: Condition objects can manage multiple synchronization points.
- **Flexibility**: Predicates allow dynamic waiting conditions.

**Disadvantages**:
- **Complexity**: Requires state and notification management, increasing error risk.
- **Overhead**: Predicate evaluation and notifications add computational cost.
- **Debugging Difficulty**: Missed notifications or state errors are hard to trace.

## Performance Comparison <a id="id5"></a>
Performance in threading solutions depends on **synchronization overhead**, **time complexity**, **space complexity**, and **context switch costs**. Since the problem involves minimal computation (printing), the primary factors are synchronization efficiency and resource usage.

### Measuring Performance 
To compare performance, we consider:
- **Time Complexity**: Computational steps, focusing on synchronization operations.
- **Space Complexity**: Memory used by synchronization objects and state variables.
- **Synchronization Overhead**: Latency from thread coordination, context switches, and polling.
- **Empirical Testing**: Execution time in a controlled environment.

#### Time Complexity <a id="id51"></a>
Each method executes a fixed number of operations per thread, with synchronization as the bottleneck. Assuming constant-time operations for printing and synchronization primitives:
- **Barrier**: O(1) operations per thread (print, wait). Total: O(1) overall.
- **Lock**: O(1) operations per thread (print, acquire, release). Total: O(1) overall.
- **Event**: O(1) operations per thread (print, set, wait). Total: O(1) overall.
- **Semaphore**: O(1) operations per thread (print, acquire, release). Total: O(1) overall.
- **Condition**: O(1) operations per thread (print, acquire, wait_for, notify). Total: O(1) overall.

**Note**: Time complexity is O(1) for all solutions due to fixed threads (3) and operations. Differences arise in synchronization overhead.

#### Space Complexity <a id="id52"></a>
- **Barrier**: Two `Barrier` objects. Space: O(1).
- **Lock**: Two `Lock` objects. Space: O(1).
- **Event**: Two `Event` objects. Space: O(1).
- **Semaphore**: Two `Semaphore` objects. Space: O(1).
- **Condition**: One `Condition` object (with RLock) and one integer. Space: O(1).

All solutions have **O(1)** space complexity due to fixed synchronization objects.

#### Synchronization Overhead <a id="id53"></a>
Synchronization overhead depends on the primitive’s implementation and operating system:
- **Barrier**: Moderate overhead due to rendezvous coordination. Threads suspend during `wait()`, minimizing CPU usage but requiring context switches.
- **Lock**: Low overhead; acquire/release is fast in uncontended cases. Fastest due to simplicity.
- **Event**: Polling in `wait()` adds slight overhead. Context switches occur when threads block.
- **Semaphore**: Slightly higher overhead than locks due to counter management, but comparable.
- **Condition**: Highest overhead due to RLock acquisition, predicate evaluation, and notifications.

#### Empirical Performance Considerations <a id="id54"></a>
In a controlled environment (e.g., Python 3.9, single-core CPU), differences are minimal due to simplicity. However:
- **Locks**: Fastest due to minimal overhead.
- **Semaphores**: Slightly slower due to counter management.
- **Events**: Slower than locks due to polling.
- **Barriers**: Moderate overhead from rendezvous.
- **Conditions**: Slowest due to predicate and notification logic.

In high-contention scenarios, locks and semaphores scale better. Barriers are less efficient with varying thread counts, and conditions are computationally intensive.

#### Performance Comparison Table <a id="id55"></a>
The following table summarizes performance characteristics:

| **Solution**  | **Time Complexity** | **Space Complexity** | **Synchronization Overhead** | **Relative Performance** | **Code Complexity** | **Maintainability** |
|---------------|---------------------|----------------------|------------------------------|--------------------------|---------------------|---------------------|
| **Barrier**   | O(1)                | O(1)                 | Moderate (rendezvous, context switches) | Moderate                | Medium              | Medium              |
| **Lock**      | O(1)                | O(1)                 | Low (fast acquire/release)   | Fastest                 | Low                 | High                |
| **Event**     | O(1)                | O(1)                 | Moderate (polling, context switches) | Moderate               | Low                 | High                |
| **Semaphore** | O(1)                | O(1)                 | Low-Moderate (counter management) | Fast                   | Medium              | Medium              |
| **Condition** | O(1)                | O(1)                 | High (predicate, notifications) | Slowest                | High                | Low                 |

**Notes**:
- **Relative Performance**: Locks are fastest; conditions are slowest.
- **Code Complexity**: Locks and events are simplest; conditions are complex.
- **Maintainability**: Locks and events are easiest to modify; conditions are error-prone.

### Complexity and Maintainability
- **Code Complexity**: Locks and events are simplest, with clear patterns. Barriers require understanding rendezvous. Semaphores add counting complexity. Conditions involve state and predicates.
- **Maintainability**: Locks and events are easiest to modify. Barriers need thread count management. Conditions demand precise state and notification handling.

## Use Case Suitability <a id="id6"></a>
- **Barrier**: Ideal for rendezvous-based synchronization (e.g., batch processing).
- **Lock**: Best for simple, low-overhead sequential synchronization.
- **Event**: Suited for signaling completion in producer-consumer patterns.
- **Semaphore**: Useful for resource counting or multiple threads.
- **Condition**: Best for complex, state-dependent synchronization.

## Conclusion <a id="id7"></a>
All solutions solve LeetCode 1114 with **O(1)** time and space complexity. **Locks** are the most efficient and simplest, ideal for this problem. **Events** offer intuitive signaling and high readability. **Semaphores** provide flexibility but are slightly less efficient. **Barriers** are intuitive for pairwise synchronization but less flexible. **Conditions** are the most complex and slowest.

For performance-critical applications, **locks** are recommended. For clarity, **events** are a strong alternative. Developers should choose based on familiarity and project requirements.

